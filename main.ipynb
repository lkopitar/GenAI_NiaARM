{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niaarm import Dataset, get_rules\n",
    "from niapy.algorithms.basic import DifferentialEvolution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%run helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSplitting data completed.\n",
      "\tOversampling completed.\n"
     ]
    }
   ],
   "source": [
    "# pina database\n",
    "df = pd.read_csv('../dataset/diabetes.csv')\n",
    "\n",
    "df[\"Outcome\"] = pd.Categorical(df['Outcome'], df['Outcome'].unique()) \n",
    "\n",
    "run = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Outcome']), df[['Outcome']],\n",
    "                                                stratify=df[['Outcome']], \n",
    "                                                test_size=0.2, \n",
    "                                                random_state= run)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "print(\"\\tSplitting data completed.\")\n",
    "\n",
    "oversample = SMOTE(random_state=run)\n",
    "\n",
    "# for Logistic regression\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train['Outcome'])\n",
    "print(\"\\tOversampling completed.\")\n",
    "\n",
    "# for GPT\n",
    "train = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "   \n",
    "## NIAARM\n",
    "train_tr = Dataset(train)\n",
    "\n",
    "algo = DifferentialEvolution(population_size=10, differential_weight=0.5, crossover_probability=0.9, random_state=run)\n",
    "metrics = ('support', 'confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tSearching for rules...\")\n",
    "association_rules_perf = pd.DataFrame(\n",
    "    {\n",
    "        \"run\": [],\n",
    "        \"nrules\": [],\n",
    "        \"fitnes\": [],\n",
    "        \"support\": [],\n",
    "        \"confidence\": []\n",
    "    }\n",
    ")\n",
    "\n",
    "while True:\n",
    "    rules, run_time = get_rules(train_tr, algo, metrics, max_iters=50, logging=False, random_state = run)\n",
    "    status = str(rules).split('\\n')\n",
    "    \n",
    "    if float(status[1].split(\":\")[1]) > 400: # ~ due to the GPT token limitation\n",
    "        continue\n",
    "    \n",
    "    # store rules to a list\n",
    "    rules_list =  []\n",
    "    for i in range(len(rules)):\n",
    "        rules_list.append(rules[i])\n",
    "\n",
    "    # only rules where Outcome is included as consequent or antecedent\n",
    "    list_of_relevant_rules = []\n",
    "    for j in range(len(rules_list)):\n",
    "        for i in range(len(getattr(rules_list[j], 'consequent'))):\n",
    "            if getattr(getattr(rules_list[j], 'consequent')[i], 'name') == 'Outcome':\n",
    "                list_of_relevant_rules.append(rules_list[j])\n",
    "                \n",
    "        for i in range(len(getattr(rules_list[j], 'antecedent'))):\n",
    "            if getattr(getattr(rules_list[j], 'antecedent')[i], 'name') == 'Outcome':\n",
    "                list_of_relevant_rules.append(rules_list[j])\n",
    "    \n",
    "    # store values of Outcome that appear within the \"list_of_relevant_rules\"            \n",
    "    values = []\n",
    "    for ls in range(len(list_of_relevant_rules)):\n",
    "        lst = getattr(list_of_relevant_rules[ls], 'consequent')\n",
    "        for i in lst:\n",
    "            if getattr(i,'name') == \"Outcome\":\n",
    "                values.append(getattr(i,'categories'))\n",
    "\n",
    "    values = [val for sublist in values for val in sublist]\n",
    "\n",
    "    # ensure there is at least one such rule with outcome as a consequent\n",
    "    if len(values) > 0:\n",
    "        # There must be at least 30% and at most 70& of class 0 and 1. We don't want to have a set of rules that represents only one single outcome\n",
    "        if ((Counter(values)[0]/len(values)) <= 0.7) and ((Counter(values)[0]/len(values)) >= 0.3) :            \n",
    "            # At least 1/10 of all rules must be rules that include an Outcome as a variable\n",
    "            if (len(rules)/10 <= len(list_of_relevant_rules)):                 \n",
    "                if (float(status[2].split(\":\")[1]) > 0.70) & (float(status[3].split(\":\")[1]) > 0.70) & (float(status[4].split(\":\")[1]) > 0.70): \n",
    "                    print(\"Found a set of rules\")\n",
    "                    print(\"\\t\",status[1])\n",
    "                    print(\"\\t\",status[2])\n",
    "                    print(\"\\t\",status[3])\n",
    "                    print(\"\\t\",status[4])\n",
    "                    row2 = {\n",
    "                        \"run\": run,\n",
    "                        \"nrules\": status[1],\n",
    "                        \"fitnes\": status[2],\n",
    "                        \"support\": status[3],\n",
    "                        \"confidence\": status[4]\n",
    "                        }\n",
    "                    association_rules_perf.loc[len(association_rules_perf)] = row2\n",
    "                    break\n",
    "print(\"\\tAssociation rule mining completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "obj = rules\n",
    "\n",
    "# TESTING RULES WITH GPT\n",
    "y_pred_gpt = []\n",
    "\n",
    "print(\"\\tTesting with GPT\")\n",
    "\n",
    "times = []\n",
    "for test_idx in range(X_test.shape[0]):\n",
    "    \n",
    "    print(\"\\t\\tRun:\"+str(run)+\" Test sample: \" + str(test_idx) + \"/\" + str(X_test.shape[0]))\n",
    "\n",
    "    test_sample = {}\n",
    "    \n",
    "    for i in range(len(X_test.loc[test_idx,].index.to_list())):\n",
    "        test_sample[X_test.loc[test_idx,].index.to_list()[i]] = X_test.loc[test_idx,].values.tolist()[i]\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": \"You will be provided with a set of rules presented in a RuleList format, each containing an antecedent and a consequent. For example, a rule might be given as [A([19, 67])] => [B([59, 98]), C([2.56, 8.3])], signifying that if A falls within the range of 19 and 67, it implies that B must fall within the range of 59 and 98, and C must fall within the range of 2.56 and 8.3. The RuleList is as follows: \"})#\"You will be provided with a set of rules. Rules will be given as a RuleList with an antecedent and consequent. [A([19, 67])] => [B([59, 98]), C([2.56, 8.3])], where if A is between 19 and 67 then B must be between 59 and 98 and C between 2.56 and 8.3. Remember the following rules: \"})\n",
    "    messages.append({\"role\": \"user\", \"content\": str(rules_list)}) \n",
    "        \n",
    "    placeholder = \"For the test sample: \"+str(test_sample)+ \" determine the 'Outcome' value. Respond with an answer in the following format 'Outcome=x' where x can be either '0' or '1'. Additionally, provide a set of rules that significantly contributed to the obtained result. Provide them in the following format 'Rules=[Rule1 ; Rule2 ; Rule3 ; Rule4]'. Rules should be written as an [antecedent] => [consequent] and should be separated by a semicolon.\"\n",
    "    messages.append({\"role\": \"user\", \"content\": placeholder})\n",
    "    \n",
    "    attempts = 5\n",
    "    attempts_predictions = []\n",
    "    attempts_replies = []\n",
    "    for i in range(attempts):   \n",
    "        reply = gpt_prompt(messages)\n",
    "        attempts_replies.append(reply)\n",
    "        attempts_predictions.append(getPrediction(reply))\n",
    "   \n",
    "    # look for majority element, if a tie then get another prediction until the answer is 0 or 1.\n",
    "    me = find_majority_element(attempts_predictions)\n",
    "    \n",
    "    # # Useless since 5 attempts\n",
    "    # if me == \"tie\":\n",
    "    #     while True:\n",
    "    #         reply = gpt_prompt(messages)\n",
    "    #         me = getPrediction(reply)\n",
    "    #         attempts_replies.append(reply)\n",
    "    #         attempts_predictions.append(me)\n",
    "    #         if me != \"tie\":\n",
    "    #             break\n",
    "    #     y_pred_gpt.append(me)\n",
    "    # else:\n",
    "    #     y_pred_gpt.append(me)\n",
    "    y_pred_gpt.append(me)\n",
    "\n",
    "    indexes = [index for index in range(len(attempts_predictions)) if attempts_predictions[index] == me]\n",
    "    explanations_raw = [attempts_replies[i] for i in indexes]\n",
    "    \n",
    "\n",
    "    for expl in explanations_raw:\n",
    "        row2 = {\n",
    "            \"run\":run,\n",
    "            \"output\": me,\n",
    "            \"rules\": expl\n",
    "        }\n",
    "        gpt_explanations.loc[len(gpt_explanations)] = row2\n",
    "    \n",
    "perf_gpt = calculate_metrics(y_test, y_pred_gpt)\n",
    "print(\"\\t\\t\", perf_gpt)\n",
    "\n",
    "row2 = {\n",
    "    \"run\": run,\n",
    "    \"Accuracy\": perf_gpt['Accuracy'],\n",
    "    \"Precision\": perf_gpt['Precision'],\n",
    "    \"F1 Score\": perf_gpt['F1 Score'],\n",
    "    \"Specificity\": perf_gpt['Specificity'],\n",
    "    \"Sensitivity\": perf_gpt['Sensitivity'],\n",
    "    \"AUC\": perf_gpt['AUC']\n",
    "}                   \n",
    "\n",
    "gpt_perf.loc[len(gpt_perf)] = row2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_list =  []\n",
    "for i in range(len(rules)):\n",
    "    rules_list.append(rules[i])\n",
    "\n",
    "counter = 0\n",
    "nth = 0\n",
    "cnt_1 = 0\n",
    "cnt_0 = 0\n",
    "for j in range(len(rules_list)):\n",
    "    for i in range(len(getattr(rules_list[j], 'consequent'))):\n",
    "        if getattr(getattr(rules_list[j], 'consequent')[i], 'name') == 'Outcome':\n",
    "            counter = counter + 1\n",
    "            \n",
    "    for i in range(len(getattr(rules_list[j], 'antecedent'))):\n",
    "        if getattr(getattr(rules_list[j], 'antecedent')[i], 'name') == 'Outcome':\n",
    "            if getattr(getattr(rules_list[j], 'antecedent')[i], 'categories')[0] == 0:\n",
    "                cnt_0 = cnt_0 + 1\n",
    "            elif getattr(getattr(rules_list[j], 'antecedent')[i], 'categories')[0] == 1:\n",
    "                cnt_1 = cnt_1 + 1\n",
    "            else:\n",
    "                nth = nth + 1\n",
    "            counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_0_outcome = gpt_explanations.loc[gpt_explanations['output']==0,'output']\n",
    "rules_1_outcome = gpt_explanations.loc[gpt_explanations['output']==1,'output']\n",
    "rules_0_outcome = rules_0_outcome.reset_index(drop=True)\n",
    "rules_1_outcome = rules_1_outcome.reset_index(drop=True)\n",
    "\n",
    "rules_0 = gpt_explanations.loc[gpt_explanations['output']==0,'rules']\n",
    "rules_1 = gpt_explanations.loc[gpt_explanations['output']==1,'rules']\n",
    "rules_0 = rules_0.reset_index(drop=True)\n",
    "rules_1 = rules_1.reset_index(drop=True)\n",
    "\n",
    "allrules_1=[]\n",
    "counter_1 = 0\n",
    "for i in range(len(rules_1)):   \n",
    "    lofrules = [item for item in re.split(r';|\\],\\[', extract_text_after_rules(rules_1[i]).replace(\" \", \"\").replace(\"[[\",\"[\").replace(\"]]\",\"]\")) if item]\n",
    "    for j in lofrules:\n",
    "        allrules_1.append(j)\n",
    "\n",
    "allrules_0=[]\n",
    "counter_0 = 0\n",
    "for i in range(len(rules_0)):   \n",
    "    lofrules = [item for item in re.split(r';|\\],\\[', extract_text_after_rules(rules_0[i]).replace(\" \", \"\").replace(\"[[\",\"[\").replace(\"]]\",\"]\")) if item]\n",
    "    for j in lofrules:\n",
    "        allrules_0.append(j)\n",
    "\n",
    "allrules_1_d = []\n",
    "for i in allrules_1:\n",
    "    allrules_1_d.append(\"[\"+i.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"=>\",\"]=>[\")+\"]\")\n",
    "    \n",
    "allrules_0_d = []\n",
    "for i in allrules_0:\n",
    "    allrules_0_d.append(\"[\"+i.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"=>\",\"]=>[\")+\"]\")  \n",
    "    \n",
    "allrules_1 = allrules_1_d\n",
    "allrules_0 = allrules_0_d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niaarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
